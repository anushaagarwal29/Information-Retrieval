{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-09T11:01:49.777747Z","iopub.status.busy":"2024-04-09T11:01:49.777455Z","iopub.status.idle":"2024-04-09T11:01:50.679293Z","shell.execute_reply":"2024-04-09T11:01:50.678300Z","shell.execute_reply.started":"2024-04-09T11:01:49.777720Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/shaojieee/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","[nltk_data] Downloading package punkt to /Users/shaojieee/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/shaojieee/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/shaojieee/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import sqlite3\n","import pandas as pd\n","from transformers import AutoTokenizer,  AutoModelForSequenceClassification, AutoConfig, get_scheduler\n","import torch.nn as nn\n","from datasets import Dataset\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","import torch\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, f1_score, precision_recall_curve, auc, roc_auc_score\n","import matplotlib.pyplot as plt\n","import os\n","\n","from preprocessing import preprocess_text"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing data"]},{"cell_type":"markdown","metadata":{},"source":["## Loading benchmark data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:01:50.680889Z","iopub.status.busy":"2024-04-09T11:01:50.680495Z","iopub.status.idle":"2024-04-09T11:02:00.574146Z","shell.execute_reply":"2024-04-09T11:02:00.573272Z","shell.execute_reply.started":"2024-04-09T11:01:50.680864Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(568454, 10)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>ProductId</th>\n","      <th>UserId</th>\n","      <th>ProfileName</th>\n","      <th>HelpfulnessNumerator</th>\n","      <th>HelpfulnessDenominator</th>\n","      <th>Score</th>\n","      <th>Time</th>\n","      <th>Summary</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>B001E4KFG0</td>\n","      <td>A3SGXH7AUHU8GW</td>\n","      <td>delmartian</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1303862400</td>\n","      <td>Good Quality Dog Food</td>\n","      <td>I have bought several of the Vitality canned d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>B00813GRG4</td>\n","      <td>A1D87F6ZCVE5NK</td>\n","      <td>dll pa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1346976000</td>\n","      <td>Not as Advertised</td>\n","      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>B000LQOCH0</td>\n","      <td>ABXLMWJIXXAIN</td>\n","      <td>Natalia Corres \"Natalia Corres\"</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1219017600</td>\n","      <td>\"Delight\" says it all</td>\n","      <td>This is a confection that has been around a fe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id   ProductId          UserId                      ProfileName  \\\n","0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n","1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n","2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n","\n","   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n","0                     1                       1      5  1303862400   \n","1                     0                       0      1  1346976000   \n","2                     1                       1      4  1219017600   \n","\n","                 Summary                                               Text  \n","0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n","1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n","2  \"Delight\" says it all  This is a confection that has been around a fe...  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["con = sqlite3.connect('./benchmarks/amazon_fine_food/database.sqlite')\n","data_train_raw_ = pd.read_sql_query(\"\"\" SELECT * FROM Reviews \"\"\", con) \n","print(data_train_raw_.shape)\n","data_train_raw_.head(3)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:02:00.576516Z","iopub.status.busy":"2024-04-09T11:02:00.576229Z","iopub.status.idle":"2024-04-09T11:02:00.592494Z","shell.execute_reply":"2024-04-09T11:02:00.591605Z","shell.execute_reply.started":"2024-04-09T11:02:00.576493Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Score\n","5    363122\n","4     80655\n","1     52268\n","3     42640\n","2     29769\n","Name: count, dtype: int64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_train_raw_['Score'].value_counts()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:02:00.593807Z","iopub.status.busy":"2024-04-09T11:02:00.593516Z","iopub.status.idle":"2024-04-09T11:02:00.620261Z","shell.execute_reply":"2024-04-09T11:02:00.619476Z","shell.execute_reply.started":"2024-04-09T11:02:00.593781Z"},"trusted":true},"outputs":[],"source":["data_train = pd.DataFrame(data_train_raw_[['Text', 'Score']])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:02:00.622118Z","iopub.status.busy":"2024-04-09T11:02:00.621607Z","iopub.status.idle":"2024-04-09T11:02:00.814414Z","shell.execute_reply":"2024-04-09T11:02:00.813652Z","shell.execute_reply.started":"2024-04-09T11:02:00.622087Z"},"trusted":true},"outputs":[],"source":["data_train['Label'] = data_train['Score'].apply(lambda x: 'POSITIVE' if x==5 else 'NEGATIVE' if x==1 else 'NEUTRAL')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:02:24.474440Z","iopub.status.busy":"2024-04-09T11:02:24.474078Z","iopub.status.idle":"2024-04-09T11:02:24.484281Z","shell.execute_reply":"2024-04-09T11:02:24.483443Z","shell.execute_reply.started":"2024-04-09T11:02:24.474404Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","      <th>Score</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I have bought several of the Vitality canned d...</td>\n","      <td>5</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n","      <td>1</td>\n","      <td>NEGATIVE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>This is a confection that has been around a fe...</td>\n","      <td>4</td>\n","      <td>NEUTRAL</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>If you are looking for the secret ingredient i...</td>\n","      <td>2</td>\n","      <td>NEUTRAL</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Great taffy at a great price.  There was a wid...</td>\n","      <td>5</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text  Score     Label\n","0  I have bought several of the Vitality canned d...      5  POSITIVE\n","1  Product arrived labeled as Jumbo Salted Peanut...      1  NEGATIVE\n","2  This is a confection that has been around a fe...      4   NEUTRAL\n","3  If you are looking for the secret ingredient i...      2   NEUTRAL\n","4  Great taffy at a great price.  There was a wid...      5  POSITIVE"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data_train.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T12:12:09.399235Z","iopub.status.busy":"2024-04-09T12:12:09.398745Z","iopub.status.idle":"2024-04-09T12:12:09.472094Z","shell.execute_reply":"2024-04-09T12:12:09.471125Z","shell.execute_reply.started":"2024-04-09T12:12:09.399197Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Label\n","POSITIVE    363122\n","NEUTRAL     153064\n","NEGATIVE     52268\n","Name: count, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data_train['Label'].value_counts()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T12:12:07.089278Z","iopub.status.busy":"2024-04-09T12:12:07.088592Z","iopub.status.idle":"2024-04-09T12:12:07.093939Z","shell.execute_reply":"2024-04-09T12:12:07.092689Z","shell.execute_reply.started":"2024-04-09T12:12:07.089245Z"},"trusted":true},"outputs":[],"source":["sentiment_encoder = {'POSITIVE': 1, 'NEGATIVE':0, 'NEUTRAL':-1}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:28.310494Z","iopub.status.busy":"2024-04-09T11:03:28.310118Z","iopub.status.idle":"2024-04-09T11:03:28.624218Z","shell.execute_reply":"2024-04-09T11:03:28.623248Z","shell.execute_reply.started":"2024-04-09T11:03:28.310465Z"},"trusted":true},"outputs":[],"source":["data_positive_train = data_train[data_train['Label']=='POSITIVE'].sample(50000, replace=False)\n","data_negative_train = data_train[data_train['Label']=='NEGATIVE'].sample(50000, replace=False)\n","\n","data_sentiment_train = pd.concat([data_positive_train, data_negative_train])\n","data_sentiment_train['class'] = data_sentiment_train['Label'].apply(lambda x:sentiment_encoder[x])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:29.579346Z","iopub.status.busy":"2024-04-09T11:03:29.578726Z","iopub.status.idle":"2024-04-09T11:03:29.611661Z","shell.execute_reply":"2024-04-09T11:03:29.610710Z","shell.execute_reply.started":"2024-04-09T11:03:29.579315Z"},"trusted":true},"outputs":[],"source":["data_sentiment_train = data_sentiment_train.rename(columns={'Text':'Review'})\n","data_sentiment_train = data_sentiment_train[['Review', 'class']]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["data_sentiment_train['Review'] = data_sentiment_train['Review'].apply(preprocess_text)"]},{"cell_type":"markdown","metadata":{},"source":["## Loading manually lablled data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:35.055855Z","iopub.status.busy":"2024-04-09T11:03:35.055534Z","iopub.status.idle":"2024-04-09T11:03:35.153564Z","shell.execute_reply":"2024-04-09T11:03:35.152705Z","shell.execute_reply.started":"2024-04-09T11:03:35.055831Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Japanese style Italian restaurant. Ikura Pasta...</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Definately visit this love'd their masala dosa</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I had a great experience at Meadesmoore! We or...</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Food was good! Service level was slightly belo...</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Food, presentation and service were all top no...</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Review     Label\n","0  Japanese style Italian restaurant. Ikura Pasta...  POSITIVE\n","1     Definately visit this love'd their masala dosa  POSITIVE\n","2  I had a great experience at Meadesmoore! We or...  POSITIVE\n","3  Food was good! Service level was slightly belo...  POSITIVE\n","4  Food, presentation and service were all top no...  POSITIVE"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["data_sentiment_test = pd.read_excel('./eval.xls')\n","data_sentiment_test.head()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:45.371074Z","iopub.status.busy":"2024-04-09T11:03:45.370421Z","iopub.status.idle":"2024-04-09T11:03:45.378560Z","shell.execute_reply":"2024-04-09T11:03:45.377538Z","shell.execute_reply.started":"2024-04-09T11:03:45.371041Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Label\n","POSITIVE    500\n","NEGATIVE    500\n","Name: count, dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["data_sentiment_test['Label'].value_counts()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:46.547800Z","iopub.status.busy":"2024-04-09T11:03:46.547419Z","iopub.status.idle":"2024-04-09T11:03:46.560157Z","shell.execute_reply":"2024-04-09T11:03:46.558786Z","shell.execute_reply.started":"2024-04-09T11:03:46.547773Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Label</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Japanese style Italian restaurant. Ikura Pasta...</td>\n","      <td>POSITIVE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Definately visit this love'd their masala dosa</td>\n","      <td>POSITIVE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I had a great experience at Meadesmoore! We or...</td>\n","      <td>POSITIVE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Food was good! Service level was slightly belo...</td>\n","      <td>POSITIVE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Food, presentation and service were all top no...</td>\n","      <td>POSITIVE</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Review     Label  class\n","0  Japanese style Italian restaurant. Ikura Pasta...  POSITIVE      1\n","1     Definately visit this love'd their masala dosa  POSITIVE      1\n","2  I had a great experience at Meadesmoore! We or...  POSITIVE      1\n","3  Food was good! Service level was slightly belo...  POSITIVE      1\n","4  Food, presentation and service were all top no...  POSITIVE      1"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["data_sentiment_test['class'] = data_sentiment_test['Label'].apply(lambda x:sentiment_encoder[x])\n","data_sentiment_test.head()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T11:03:52.749988Z","iopub.status.busy":"2024-04-09T11:03:52.749203Z","iopub.status.idle":"2024-04-09T11:03:52.754853Z","shell.execute_reply":"2024-04-09T11:03:52.753859Z","shell.execute_reply.started":"2024-04-09T11:03:52.749950Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>japanese style italian restaurant ikura pasta ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>definately visit loved masala dosa</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>great experience meadesmoore ordered ox tongue...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>food good service level slightly expectation n...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>food presentation service top notch also solid...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              Review  class\n","0  japanese style italian restaurant ikura pasta ...      1\n","1                 definately visit loved masala dosa      1\n","2  great experience meadesmoore ordered ox tongue...      1\n","3  food good service level slightly expectation n...      1\n","4  food presentation service top notch also solid...      1"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["data_sentiment_test = data_sentiment_test[['Review', 'class']]\n","data_sentiment_test['Review'] = data_sentiment_test['Review'].apply(preprocess_text)\n","data_sentiment_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Sentiment Training"]},{"cell_type":"markdown","metadata":{},"source":["## Data Prep"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:16:02.725660Z","iopub.status.busy":"2024-04-09T07:16:02.725197Z","iopub.status.idle":"2024-04-09T07:16:06.552739Z","shell.execute_reply":"2024-04-09T07:16:06.551765Z","shell.execute_reply.started":"2024-04-09T07:16:02.725633Z"},"trusted":true},"outputs":[],"source":["MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","def tokenize_function(examples, col):\n","    return tokenizer(examples[col], padding='max_length', truncation=True, max_length=512,return_tensors='pt')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:16:06.554172Z","iopub.status.busy":"2024-04-09T07:16:06.553902Z","iopub.status.idle":"2024-04-09T07:16:06.922661Z","shell.execute_reply":"2024-04-09T07:16:06.921758Z","shell.execute_reply.started":"2024-04-09T07:16:06.554148Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset.from_pandas(data_sentiment_train)\n","test_dataset = Dataset.from_pandas(data_sentiment_test)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:16:06.925494Z","iopub.status.busy":"2024-04-09T07:16:06.925045Z","iopub.status.idle":"2024-04-09T07:17:01.199869Z","shell.execute_reply":"2024-04-09T07:17:01.198913Z","shell.execute_reply.started":"2024-04-09T07:16:06.925458Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 100000/100000 [00:20<00:00, 4908.34 examples/s]\n","Map: 100%|██████████| 1000/1000 [00:00<00:00, 5363.00 examples/s]\n"]}],"source":["train_dataset = train_dataset.map(lambda x:tokenize_function(x,'Review'), batched=True)\n","test_dataset = test_dataset.map(lambda x:tokenize_function(x,'Review'), batched=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:17:01.201471Z","iopub.status.busy":"2024-04-09T07:17:01.201120Z","iopub.status.idle":"2024-04-09T07:17:01.215156Z","shell.execute_reply":"2024-04-09T07:17:01.214206Z","shell.execute_reply.started":"2024-04-09T07:17:01.201437Z"},"trusted":true},"outputs":[],"source":["def format_tokenized_datasets(dataset):\n","    dataset = dataset.remove_columns(['Review'])\n","    dataset = dataset.rename_column('class', 'labels')\n","    dataset.set_format('torch')\n","    return dataset\n","\n","train_dataset = format_tokenized_datasets(train_dataset)\n","test_dataset = format_tokenized_datasets(test_dataset)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:17:01.217499Z","iopub.status.busy":"2024-04-09T07:17:01.217172Z","iopub.status.idle":"2024-04-09T07:17:01.267669Z","shell.execute_reply":"2024-04-09T07:17:01.266723Z","shell.execute_reply.started":"2024-04-09T07:17:01.217474Z"},"trusted":true},"outputs":[],"source":["indexes = np.arange(len(train_dataset))\n","np.random.shuffle(indexes)\n","train_index = indexes[:int(0.8*len(train_dataset))]\n","val_index = indexes[int(0.8*len(train_dataset)):]\n","\n","train_dataloader = DataLoader(train_dataset.select(train_index), shuffle=True, batch_size=8)\n","val_dataloader = DataLoader(train_dataset.select(val_index), shuffle=True, batch_size=8)\n","test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=8)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Prep"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T07:17:01.269206Z","iopub.status.busy":"2024-04-09T07:17:01.268886Z","iopub.status.idle":"2024-04-09T07:17:01.298775Z","shell.execute_reply":"2024-04-09T07:17:01.297826Z","shell.execute_reply.started":"2024-04-09T07:17:01.269180Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","elif torch.backends.mps.is_built():\n","    device = 'mps'\n","else:\n","    device = 'cpu'\n","    \n","device = torch.device(device)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T08:36:09.126118Z","iopub.status.busy":"2024-04-09T08:36:09.125703Z","iopub.status.idle":"2024-04-09T08:36:09.131891Z","shell.execute_reply":"2024-04-09T08:36:09.130998Z","shell.execute_reply.started":"2024-04-09T08:36:09.126086Z"},"trusted":true},"outputs":[],"source":["def build_model():\n","    config = AutoConfig.from_pretrained(MODEL)\n","    model =  AutoModelForSequenceClassification.from_pretrained(MODEL, config=config)\n","    model.classifier.out_proj = nn.Sequential(\n","        nn.Linear(\n","            in_features=768,\n","            out_features=2,\n","            bias=True\n","        ),\n","        nn.Softmax()\n","    )\n","\n","    # for param in model.roberta.parameters():\n","    #     param.requires_grad=False\n","\n","    # for name, param in model.named_parameters():\n","    #     if param.requires_grad:\n","    #         print(name)\n","\n","    return model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T08:36:55.358173Z","iopub.status.busy":"2024-04-09T08:36:55.357784Z","iopub.status.idle":"2024-04-09T08:36:55.974625Z","shell.execute_reply":"2024-04-09T08:36:55.973655Z","shell.execute_reply.started":"2024-04-09T08:36:55.358142Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["num_epochs = 20\n","num_training_steps = num_epochs * len(train_dataloader)\n","model = build_model()\n","optimizer = Adam(model.parameters(), lr=5e-5)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")\n","\n","loss_fn = torch.nn.BCELoss()\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Sentiment Training on benchmark dataset"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T08:37:12.042607Z","iopub.status.busy":"2024-04-09T08:37:12.042266Z","iopub.status.idle":"2024-04-09T09:34:54.778742Z","shell.execute_reply":"2024-04-09T09:34:54.777748Z","shell.execute_reply.started":"2024-04-09T08:37:12.042581Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10000 [00:00<?, ?it/s]/Users/shaojieee/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n","  0%|          | 5/10000 [00:48<27:04:05,  9.75s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     14\u001b[0m target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mlogits, target)\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:197\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    189\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 197\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results = {'precision':[], 'recall':[], 'f1': []}\n","best_f1 = 0\n","max_patience = 2\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    model.train()\n","    samples = 0\n","    for batch in tqdm(train_dataloader):\n","        if samples>=10000:\n","            break\n","        samples += 8\n","        inputs = {k: v.to(device) for k, v in batch.items() if k!='labels' and k!='__index_level_0__'}\n","        target = F.one_hot(batch['labels'], num_classes=2).to(torch.float).to(device)\n","        outputs = model(**inputs)\n","        loss = loss_fn(outputs.logits, target)\n","        total_loss += loss.item()\n","        \n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    print(total_loss/len(train_dataloader))\n","\n","    model.eval()\n","    pred = []\n","    true = []\n","    with torch.no_grad():\n","        samples = 0\n","        for batch in tqdm(val_dataloader):\n","            inputs = {k: v.to(device) for k, v in batch.items() if k!='labels' and k!='__index_level_0__'}\n","            target = F.one_hot(batch['labels'], num_classes=2).to(torch.float)\n","            outputs = model(**inputs).logits.detach().cpu().numpy()\n","\n","            pred.append(outputs[:, 1])\n","            true.append(batch['labels'].numpy())\n","\n","        true = np.hstack(true)\n","        pred = np.hstack(pred)\n","        precision = precision_score(true, np.round(pred), average='macro')\n","        recall = recall_score(true, np.round(pred), average='macro')\n","        f1 = f1_score(true, np.round(pred), average='macro')\n","        print(\"Precision:\", precision)\n","        print(\"Recall:\", recall)\n","        print(\"F-measure:\", f1)\n","        \n","        if f1>best_f1:\n","            best_f1 = f1\n","            patience = 0\n","            best_weights = model.state_dict()\n","        else:\n","            patience += 1\n","        \n","        if patience==max_patience:\n","            model.load_state_dict(best_weights)\n","            break\n","            \n","        results['precision'].append(precision)\n","        results['recall'].append(recall)\n","        results['f1'].append(f1)\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T09:34:58.007996Z","iopub.status.busy":"2024-04-09T09:34:58.007614Z","iopub.status.idle":"2024-04-09T09:34:58.725375Z","shell.execute_reply":"2024-04-09T09:34:58.724377Z","shell.execute_reply.started":"2024-04-09T09:34:58.007959Z"},"trusted":true},"outputs":[],"source":["# Save the model's state_dict\n","# os.makedirs('model_weights', exist_ok=True)\n","# torch.save(model.state_dict(), 'sentiment_model_weights.pth')"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["new_model = build_model()\n","new_model.load_state_dict(torch.load('./model_weights/sentiment_clean_model_weights.pth',map_location=torch.device('cpu')))\n","new_model = new_model.to(device)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 7/2500 [00:10<1:00:10,  1.45s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      7\u001b[0m target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     10\u001b[0m pred\u001b[38;5;241m.\u001b[39mappend(outputs[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m true\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:220\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 220\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    222\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    224\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["new_model.eval()\n","pred = []\n","true = []\n","with torch.no_grad():\n","    for batch in tqdm(val_dataloader):\n","        inputs = {k: v.to(device) for k, v in batch.items() if k!='labels' and k!='__index_level_0__'}\n","        target = F.one_hot(batch['labels'], num_classes=2).to(torch.float)\n","        outputs = new_model(**inputs).logits.detach().cpu().numpy()\n","\n","        pred.append(outputs[:, 1])\n","        true.append(batch['labels'].numpy())\n","\n","    true = np.hstack(true)\n","    pred = np.hstack(pred)\n","    precision = precision_score(true, np.round(pred), average='macro')\n","    recall = recall_score(true, np.round(pred), average='macro')\n","    f1 = f1_score(true, np.round(pred), average='macro')\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F-measure:\", f1)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["cm = confusion_matrix(true, np.round(pred))\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.93      0.98      0.95     10055\n","           1       0.98      0.92      0.95      9945\n","\n","    accuracy                           0.95     20000\n","   macro avg       0.95      0.95      0.95     20000\n","weighted avg       0.95      0.95      0.95     20000\n","\n"]}],"source":["print(classification_report(true, np.round(pred)))"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["0.9901391965367257"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["precision, recall, thresholds = precision_recall_curve(true, pred)\n","pr_auc = auc(recall, precision)\n","pr_auc"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["0.9908798891166457"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["roc_auc_score(true, pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate on manually labelled data"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["new_model = build_model()\n","new_model.load_state_dict(torch.load('./model_weights/sentiment_clean_model_weights.pth',map_location=torch.device('cpu')))\n","new_model = new_model.to(device)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T09:34:54.781233Z","iopub.status.busy":"2024-04-09T09:34:54.780768Z","iopub.status.idle":"2024-04-09T09:34:58.006175Z","shell.execute_reply":"2024-04-09T09:34:58.005162Z","shell.execute_reply.started":"2024-04-09T09:34:54.781197Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/125 [00:00<?, ?it/s]/Users/shaojieee/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n","100%|██████████| 125/125 [00:03<00:00, 31.88it/s]"]},{"name":"stdout","output_type":"stream","text":["Precision: 0.9436406170510216\n","Recall: 0.9430000000000001\n","F-measure: 0.9429794155690204\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["new_model.eval()\n","pred = []\n","true = []\n","with torch.no_grad():\n","    for batch in tqdm(test_dataloader):\n","        inputs = {k: v.to(device) for k, v in batch.items() if k!='labels' and k!='__index_level_0__'}\n","        target = F.one_hot(batch['labels'], num_classes=2).to(torch.float)\n","        outputs = new_model(**inputs).logits.detach().cpu().numpy()\n","\n","        pred.append(outputs[:, 1])\n","        true.append(batch['labels'].numpy())\n","\n","    true = np.hstack(true)\n","    pred = np.hstack(pred)\n","    precision = precision_score(true, np.round(pred), average='macro')\n","    recall = recall_score(true, np.round(pred), average='macro')\n","    f1 = f1_score(true, np.round(pred), average='macro')\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F-measure:\", f1)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T09:39:20.711786Z","iopub.status.busy":"2024-04-09T09:39:20.711273Z","iopub.status.idle":"2024-04-09T09:39:20.983083Z","shell.execute_reply":"2024-04-09T09:39:20.982078Z","shell.execute_reply.started":"2024-04-09T09:39:20.711745Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["cm = confusion_matrix(true, np.round(pred))\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","plt.show()"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T09:39:29.105720Z","iopub.status.busy":"2024-04-09T09:39:29.104839Z","iopub.status.idle":"2024-04-09T09:39:29.120881Z","shell.execute_reply":"2024-04-09T09:39:29.119793Z","shell.execute_reply.started":"2024-04-09T09:39:29.105685Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.92      0.94       500\n","           1       0.93      0.96      0.94       500\n","\n","    accuracy                           0.94      1000\n","   macro avg       0.94      0.94      0.94      1000\n","weighted avg       0.94      0.94      0.94      1000\n","\n"]}],"source":["print(classification_report(true, np.round(pred)))"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["0.9666892294816195"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["precision, recall, thresholds = precision_recall_curve(true, pred)\n","pr_auc = auc(recall, precision)\n","pr_auc"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["0.980876"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["roc_auc_score(true, pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Predicting on crawled data"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Rating</th>\n","      <th>Review</th>\n","      <th>Restaurant</th>\n","      <th>Business_Status</th>\n","      <th>Formatted_Address</th>\n","      <th>Place_Id</th>\n","      <th>Types</th>\n","      <th>User_Ratings_Total</th>\n","      <th>Latlng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Loved the ambience !</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Food was great . Salted baked fish , prawns an...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>The food is spectacular. We were sharing about...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Yummy authentic Greek food. The dips were real...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Pretty good food. =)</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         Name  Rating  \\\n","0  Alati Divine Greek Cuisine     4.5   \n","1  Alati Divine Greek Cuisine     4.5   \n","2  Alati Divine Greek Cuisine     4.5   \n","3  Alati Divine Greek Cuisine     4.5   \n","4  Alati Divine Greek Cuisine     4.5   \n","\n","                                              Review  \\\n","0                               Loved the ambience !   \n","1  Food was great . Salted baked fish , prawns an...   \n","2  The food is spectacular. We were sharing about...   \n","3  Yummy authentic Greek food. The dips were real...   \n","4                               Pretty good food. =)   \n","\n","                   Restaurant Business_Status             Formatted_Address  \\\n","0  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","1  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","2  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","3  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","4  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","\n","                      Place_Id  \\\n","0  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","1  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","2  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","3  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","4  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","\n","                                             Types  User_Ratings_Total  \\\n","0  restaurant,food,point_of_interest,establishment                 695   \n","1  restaurant,food,point_of_interest,establishment                 695   \n","2  restaurant,food,point_of_interest,establishment                 695   \n","3  restaurant,food,point_of_interest,establishment                 695   \n","4  restaurant,food,point_of_interest,establishment                 695   \n","\n","                  Latlng  \n","0  1.2803412,103.8467476  \n","1  1.2803412,103.8467476  \n","2  1.2803412,103.8467476  \n","3  1.2803412,103.8467476  \n","4  1.2803412,103.8467476  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('../crawling/raw-v2/combined.csv')\n","data.head()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["data['Review_Cleaned'] = data['Review'].apply(preprocess_text)\n","dataset = pd.DataFrame(data[['Review_Cleaned']])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","def tokenize_function(examples, col):\n","    return tokenizer(examples[col], padding=True, truncation=True, max_length=512,return_tensors='pt')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["dataset = Dataset.from_pandas(dataset)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 59191/59191 [00:01<00:00, 33970.98 examples/s]\n"]}],"source":["dataset = dataset.map(lambda x:tokenize_function(x,'Review_Cleaned'), batched=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["dataset = dataset.remove_columns(['Review_Cleaned'])\n","dataset.set_format('torch')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["dataloader = DataLoader(dataset, shuffle=False, batch_size=8)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["new_model = build_model()\n","new_model.load_state_dict(torch.load('./model_weights/sentiment_clean_model_weights.pth',map_location=torch.device('cpu')))\n","new_model = new_model.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7399 [00:00<?, ?it/s]/Users/shaojieee/.pyenv/versions/3.10.11/envs/info_retrieval/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self._call_impl(*args, **kwargs)\n","100%|██████████| 7399/7399 [03:27<00:00, 35.73it/s]\n"]}],"source":["new_model.eval()\n","pred = []\n","with torch.no_grad():\n","    for batch in tqdm(dataloader):\n","        inputs = {k: v.to(device) for k, v in batch.items() if k!='labels' and k!='__index_level_0__'}\n","        outputs = new_model(**inputs).logits.detach().cpu().numpy()\n","        pred.append(outputs[:, 1])\n","        \n","    pred = np.hstack(pred)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Rating</th>\n","      <th>Review</th>\n","      <th>Restaurant</th>\n","      <th>Business_Status</th>\n","      <th>Formatted_Address</th>\n","      <th>Place_Id</th>\n","      <th>Types</th>\n","      <th>User_Ratings_Total</th>\n","      <th>Latlng</th>\n","      <th>Review_Cleaned</th>\n","      <th>polarity_probability</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Loved the ambience !</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","      <td>loved ambience</td>\n","      <td>0.989528</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Food was great . Salted baked fish , prawns an...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","      <td>food great salted baked fish prawn dip size fi...</td>\n","      <td>0.484377</td>\n","      <td>NEGATIVE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>The food is spectacular. We were sharing about...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","      <td>food spectacular sharing 5 dish every single o...</td>\n","      <td>0.998068</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Yummy authentic Greek food. The dips were real...</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","      <td>yummy authentic greek food dip really tasty go...</td>\n","      <td>0.998058</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>4.5</td>\n","      <td>Pretty good food. =)</td>\n","      <td>Alati Divine Greek Cuisine</td>\n","      <td>OPERATIONAL</td>\n","      <td>73 Amoy St, Singapore 069892</td>\n","      <td>ChIJn0mYZQ0Z2jERomxAoOxa4zI</td>\n","      <td>restaurant,food,point_of_interest,establishment</td>\n","      <td>695</td>\n","      <td>1.2803412,103.8467476</td>\n","      <td>pretty good food</td>\n","      <td>0.997312</td>\n","      <td>POSITIVE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         Name  Rating  \\\n","0  Alati Divine Greek Cuisine     4.5   \n","1  Alati Divine Greek Cuisine     4.5   \n","2  Alati Divine Greek Cuisine     4.5   \n","3  Alati Divine Greek Cuisine     4.5   \n","4  Alati Divine Greek Cuisine     4.5   \n","\n","                                              Review  \\\n","0                               Loved the ambience !   \n","1  Food was great . Salted baked fish , prawns an...   \n","2  The food is spectacular. We were sharing about...   \n","3  Yummy authentic Greek food. The dips were real...   \n","4                               Pretty good food. =)   \n","\n","                   Restaurant Business_Status             Formatted_Address  \\\n","0  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","1  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","2  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","3  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","4  Alati Divine Greek Cuisine     OPERATIONAL  73 Amoy St, Singapore 069892   \n","\n","                      Place_Id  \\\n","0  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","1  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","2  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","3  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","4  ChIJn0mYZQ0Z2jERomxAoOxa4zI   \n","\n","                                             Types  User_Ratings_Total  \\\n","0  restaurant,food,point_of_interest,establishment                 695   \n","1  restaurant,food,point_of_interest,establishment                 695   \n","2  restaurant,food,point_of_interest,establishment                 695   \n","3  restaurant,food,point_of_interest,establishment                 695   \n","4  restaurant,food,point_of_interest,establishment                 695   \n","\n","                  Latlng                                     Review_Cleaned  \\\n","0  1.2803412,103.8467476                                     loved ambience   \n","1  1.2803412,103.8467476  food great salted baked fish prawn dip size fi...   \n","2  1.2803412,103.8467476  food spectacular sharing 5 dish every single o...   \n","3  1.2803412,103.8467476  yummy authentic greek food dip really tasty go...   \n","4  1.2803412,103.8467476                                   pretty good food   \n","\n","   polarity_probability  polarity  \n","0              0.989528  POSITIVE  \n","1              0.484377  NEGATIVE  \n","2              0.998068  POSITIVE  \n","3              0.998058  POSITIVE  \n","4              0.997312  POSITIVE  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["data['polarity_probability'] = pred\n","data['polarity'] = np.round(pred)\n","\n","data['polarity'] = data['polarity'].replace({1:'POSITIVE', 0:'NEGATIVE'})\n","\n","data.head()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["data.to_csv('./results/clean_reviews_labelled.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":18,"sourceId":2157,"sourceType":"datasetVersion"},{"datasetId":4761668,"sourceId":8072158,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
