{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features and descriptive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 12.5MB/s]                    \n",
      "2024-04-13 16:21:48 INFO: Downloaded file to /Users/shaojieee/stanza_resources/resources.json\n",
      "2024-04-13 16:21:48 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-13 16:21:49 INFO: File exists: /Users/shaojieee/stanza_resources/en/default.zip\n",
      "2024-04-13 16:21:52 INFO: Finished downloading models and saved to /Users/shaojieee/stanza_resources\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shaojieee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shaojieee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shaojieee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import stanza\n",
    "from preprocessing import remove_emojis, handle_contractions, handle_ellipses, handle_apostrophes,lowercase, remove_punctuation\n",
    "\n",
    "\n",
    "\n",
    "# download English model\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 20:36:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 26.4MB/s]                    \n",
      "2024-04-12 20:36:10 INFO: Downloaded file to /Users/shaojieee/stanza_resources/resources.json\n",
      "2024-04-12 20:36:11 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-04-12 20:36:11 INFO: Using device: cpu\n",
      "2024-04-12 20:36:11 INFO: Loading: tokenize\n",
      "2024-04-12 20:36:11 INFO: Loading: mwt\n",
      "2024-04-12 20:36:11 INFO: Loading: pos\n",
      "2024-04-12 20:36:11 INFO: Loading: lemma\n",
      "2024-04-12 20:36:11 INFO: Loading: constituency\n",
      "2024-04-12 20:36:11 INFO: Loading: depparse\n",
      "2024-04-12 20:36:12 INFO: Loading: sentiment\n",
      "2024-04-12 20:36:12 INFO: Loading: ner\n",
      "2024-04-12 20:36:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "\n",
    "    txt = txt.lower() # LowerCasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt) # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "    try:\n",
    "        for line in sentList:\n",
    "            txt_list = nltk.word_tokenize(line) # Splitting up into words\n",
    "            taggedList = nltk.pos_tag(txt_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "            newwordList = []\n",
    "            flag = 0\n",
    "            for i in range(0,len(taggedList)-1):\n",
    "                if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                    newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                    flag=1\n",
    "                else:\n",
    "                    if(flag==1):\n",
    "                        flag=0\n",
    "                        continue\n",
    "                    newwordList.append(taggedList[i][0])\n",
    "                    if(i==len(taggedList)-2):\n",
    "                        newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "            \n",
    "            finaltxt = ' '.join(word for word in newwordList) \n",
    "            new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "            wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "            taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "            doc = nlp(finaltxt) \n",
    "            # Getting the dependency relations between the words\n",
    "            dep_node = []\n",
    "            for dep_edge in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "            # Coverting it into appropriate format\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "            featureList = []\n",
    "            categories = []\n",
    "            for i in taggedList:\n",
    "                if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                    featureList.append(list(i)) # For features for each sentence\n",
    "                    totalfeatureList.append(list(i)) # Stores the features of all the sentences in the text\n",
    "                    categories.append(i[0])\n",
    "\n",
    "            for i in featureList:\n",
    "                filist = []\n",
    "                for j in dep_node:\n",
    "                    if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                        if(j[0]==i[0]):\n",
    "                            filist.append(j[1])\n",
    "                        else:\n",
    "                            filist.append(j[0])\n",
    "                fcluster.append([i[0], filist])\n",
    "                \n",
    "        for i in totalfeatureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        \n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "            \n",
    "        return(finalcluster)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "nlp = stanza.Pipeline()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = lowercase(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = handle_contractions(text)\n",
    "    text = handle_ellipses(text)\n",
    "    text = handle_apostrophes(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['service', ['shocking']], ['bill', ['astronomical']]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"Service shocking had to ask for 2 beers twice then got presented with astronomical bill I.e 30 dollars for 2 small beers won't be back soon.\"\n",
    "txt = preprocess_text(txt)\n",
    "aspect_sentiment_analysis(txt, stop_words, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./eval.xls')\n",
    "data['Review_Clean'] = data['Review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['absa'] = data['Review_Clean'].apply(lambda x:aspect_sentiment_analysis(x, stop_words, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Food was hands down beating expectations. I knew it would be amazing for a Michelin star restaurant but I actually thought it was better quality than just a 1 star. Also appreciated the large diversity in courses and in between tasters for …',\n",
       "        'POSITIVE',\n",
       "        'food was hands down beating expectations. i knew it would be amazing for a michelin star restaurant but i actually thought it was better quality than just a 1 star. also appreciated the large diversity in courses and in between tasters for  ',\n",
       "        list([['food', ['beating']], ['michelinstar', ['starrestaurant']], ['starrestaurant', ['michelinstar']], ['quality', ['it', 'better']], ['star', ['just']], ['diversity', ['large', 'appreciated']]])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./results/absa_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([['Nice rooftop terrace with a view, but that’s it. Very poor service and overpriced food for low quality. Overall disappointing experience.',\n",
    "#         'NEGATIVE',\n",
    "#         'nice rooftop terrace with a view, but that s it. very poor service and overpriced food for low quality. overall disappointing experience.',\n",
    "#         list([['rooftopterrace', ['nice']], ['view', []], ['service', ['poor']], ['food', ['overpriced']], ['quality', ['low']], ['experience', ['overall', 'disappointing']]])]],\n",
    "#       dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([['Incredible food and amazing service!\\n\\nWe came for the chili crab and it was delicious! The soups are also very good. …',\n",
    "#         'POSITIVE',\n",
    "#         'incredible food and amazing service!\\n\\nwe came for the chili crab and it was delicious! the soups are also very good.  ',\n",
    "#         list([['food', ['incredible']], ['service', ['amazing']]])]],\n",
    "#       dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying aspect into polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./results/absa_results.csv', converters={'absa': pd.eval}, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "      <th>Review_Clean</th>\n",
       "      <th>absa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japanese style Italian restaurant. Ikura Pasta...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>japanese style italian restaurant. ikura pasta...</td>\n",
       "      <td>[[style, [restaurant]], [restaurant, [japanese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Definately visit this love'd their masala dosa</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>definately visit this love'd their masala dosa</td>\n",
       "      <td>[[visit, [definately, love]], [love, [visit]],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had a great experience at Meadesmoore! We or...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>i had a great experience at meadesmoore! we or...</td>\n",
       "      <td>[[experience, [great, had]], [meadesmoore, []]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Food was good! Service level was slightly belo...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>food was good! service level was slightly belo...</td>\n",
       "      <td>[[food, [good]], [good, [food]], [servicelevel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food, presentation and service were all top no...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>food, presentation and service were all top no...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review     Label  \\\n",
       "0  Japanese style Italian restaurant. Ikura Pasta...  POSITIVE   \n",
       "1     Definately visit this love'd their masala dosa  POSITIVE   \n",
       "2  I had a great experience at Meadesmoore! We or...  POSITIVE   \n",
       "3  Food was good! Service level was slightly belo...  POSITIVE   \n",
       "4  Food, presentation and service were all top no...  POSITIVE   \n",
       "\n",
       "                                        Review_Clean  \\\n",
       "0  japanese style italian restaurant. ikura pasta...   \n",
       "1     definately visit this love'd their masala dosa   \n",
       "2  i had a great experience at meadesmoore! we or...   \n",
       "3  food was good! service level was slightly belo...   \n",
       "4  food, presentation and service were all top no...   \n",
       "\n",
       "                                                absa  \n",
       "0  [[style, [restaurant]], [restaurant, [japanese...  \n",
       "1  [[visit, [definately, love]], [love, [visit]],...  \n",
       "2  [[experience, [great, had]], [meadesmoore, []]...  \n",
       "3  [[food, [good]], [good, [food]], [servicelevel...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/shaojieee/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "def calculate_sentiment_score(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    for token in tokens:\n",
    "        synsets = list(swn.senti_synsets(token))\n",
    "        if synsets:\n",
    "            pos_score = sum(s.pos_score() for s in synsets) / len(synsets)\n",
    "            neg_score = sum(s.neg_score() for s in synsets) / len(synsets)\n",
    "            positive_score += pos_score\n",
    "            negative_score += neg_score\n",
    "    if positive_score > negative_score:\n",
    "        return 'POSITIVE'\n",
    "    elif positive_score < negative_score:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'NEUTRAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentiment(absa):\n",
    "    polarities = []\n",
    "    for x in absa:\n",
    "        feature, desc = x\n",
    "        if len(desc)>0:\n",
    "            polarity = calculate_sentiment_score(' '.join(desc))\n",
    "            polarities.append([feature, polarity])\n",
    "    \n",
    "    return polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity'] = data['absa'].apply(convert_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./results/absa_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
